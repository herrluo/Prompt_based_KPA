{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['topic', 'stance', 'arguments', 'kep_points', 'predict_kps'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "gt_df = pd.read_csv('./result/ArgKP21+predictions(v2).csv')\n",
    "print(gt_df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n",
      "Assisted suicide allows people to solicit someone to die to their own benefit\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "gt = gt_df['kep_points'].values.tolist()\n",
    "gt_list = []\n",
    "for sentence_list in gt:\n",
    "    gt_list += ast.literal_eval(sentence_list)\n",
    "print(len(gt_list))\n",
    "print(gt_list[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['topic', 'stance', 'arguments', 'kep_points', 'predict_kps'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "gpt4_group_df = pd.read_csv('./result/ArgKP21+predictions(v2).csv')\n",
    "print(gpt4_group_df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n",
      "Assisted suicide could potentially open the door for individuals with ulterior motives to coerce or manipulate vulnerable patients into ending their lives.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "gpt4_group = gpt4_group_df['predict_kps'].values.tolist()\n",
    "gpt4_group_list = []\n",
    "for sentence_list in gpt4_group:\n",
    "    gpt4_group_list += ast.literal_eval(sentence_list)\n",
    "print(len(gpt4_group_list))\n",
    "print(gpt4_group_list[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['topic', 'stance', 'arguments', 'kep_points',\n",
      "       'predict_kps(Bert_embedding)', 'predict_kps(avg_embedding)',\n",
      "       'predict_kps(best_embedding)', 'predict_kps(PCA_embedding)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "gpt4_1by1_df = pd.read_csv('./result/gpt4_1by1+predictions_in_group.csv')\n",
    "print(gpt4_1by1_df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n",
      "Assisted suicide equates to facilitating murder and should therefore be criminalized.\n"
     ]
    }
   ],
   "source": [
    "gpt4_1by1 = gpt4_1by1_df['predict_kps(PCA_embedding)'].values.tolist()\n",
    "gpt4_1by1_list = []\n",
    "for sentence_list in gpt4_1by1:\n",
    "    gpt4_1by1_list += ast.literal_eval(sentence_list)\n",
    "print(len(gpt4_1by1_list))\n",
    "print(gpt4_1by1_list[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['topic', 'stance', 'arguments', 'kep_points', 'predict_kps'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "llama2_group_df = pd.read_csv('./LLama2/data/ArgKP21+predictions(v3).csv')\n",
    "print(llama2_group_df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n",
      "Assisted suicide should be a criminal offense because it involves taking a life, which is morally and ethically wrong.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "llama2_group = llama2_group_df['predict_kps'].values.tolist()\n",
    "llama2_group_list = []\n",
    "for sentence_list in llama2_group:\n",
    "    llama2_group_list += ast.literal_eval(sentence_list)\n",
    "print(len(llama2_group_list))\n",
    "print(llama2_group_list[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['topic', 'stance', 'arguments', 'kep_points',\n",
      "       'predict_kps(avg_embedding)', 'predict_kps(best_embedding)',\n",
      "       'predict_kps(PCA_embedding)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "llama2_1by1_df = pd.read_csv('./LLama2/data/llama2_1by1+predictions_in_group.csv')\n",
    "print(llama2_1by1_df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n",
      "Assisted suicide should be a criminal offense because it can lead to exploitation and murder of vulnerable individuals who may not have the capacity to make informed decisions.\n"
     ]
    }
   ],
   "source": [
    "llama2_1by1 = llama2_1by1_df['predict_kps(avg_embedding)'].values.tolist()\n",
    "llama2_1by1_list = []\n",
    "for sentence_list in llama2_1by1:\n",
    "    llama2_1by1_list += ast.literal_eval(sentence_list)\n",
    "print(len(llama2_1by1_list))\n",
    "print(llama2_1by1_list[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['topic', 'stance', 'arguments', 'kep_points', 'predict_kps'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "qwen_group_df = pd.read_csv('./qwen/data/ArgKP21+predictions(v4).csv')\n",
    "print(qwen_group_df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n",
      "assisted suicide is a form of murder and should be treated as a crime\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "qwen_group = qwen_group_df['predict_kps'].values.tolist()\n",
    "qwen_group_list = []\n",
    "for sentence_list in qwen_group:\n",
    "    qwen_group_list += ast.literal_eval(sentence_list)\n",
    "print(len(qwen_group_list))\n",
    "print(qwen_group_list[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['topic', 'stance', 'arguments', 'kep_points',\n",
      "       'predict_kps(avg_embedding)', 'predict_kps(best_embedding)',\n",
      "       'predict_kps(PCA_embedding)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "qwen_1by1_df = pd.read_csv('./qwen/data/qwen_1by1+predictions_in_group.csv')\n",
    "print(qwen_1by1_df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n",
      "Assisted suicide constitutes glorified murder, hence it should be a criminal offence, with focus on providing support for better coping mechanisms.\n"
     ]
    }
   ],
   "source": [
    "qwen_1by1 = qwen_1by1_df['predict_kps(avg_embedding)'].values.tolist()\n",
    "qwen_1by1_list = []\n",
    "for sentence_list in qwen_1by1:\n",
    "    qwen_1by1_list += ast.literal_eval(sentence_list)\n",
    "print(len(qwen_1by1_list))\n",
    "print(qwen_1by1_list[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#gt_list,gpt4_group_list,gpt4_1by1_list,llama2_group_list,llama2_1by1_list,qwen_group_list,qwen_1by1_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Program Files\\KPA prompt-based\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.01s/it]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "G:\\Program Files\\KPA prompt-based\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.3333333333333333"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BLANC\n",
    "from blanc import BlancHelp, BlancTune\n",
    "document = \"Jack drove his minivan to the bazaar to purchase milk and honey for his large family.\"\n",
    "summary = \"Jack bought milk and honey.\"\n",
    "blanc_help = BlancHelp()\n",
    "blanc_tune = BlancTune(finetune_mask_evenly=False, show_progress_bar=False)\n",
    "blanc_help.eval_once(document, summary)\n",
    "0.2222222222222222\n",
    "# blanc_tune.eval_once(document, summary)\n",
    "0.3333333333333333\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average FKGL: 14.715483870967745\n",
      "Average GFI: 18.53793548387097\n",
      "Average FRE: 18.17370967741936\n"
     ]
    }
   ],
   "source": [
    "# FKGL,FRE, GFI\n",
    "import textstat\n",
    "\n",
    "def evaluate_readability_metrics(sentences):\n",
    "    total_fkgl = 0\n",
    "    total_gfi = 0\n",
    "    total_fre = 0\n",
    "    num_sentences = len(sentences)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        total_fkgl += textstat.flesch_kincaid_grade(sentence)\n",
    "        total_gfi += textstat.gunning_fog(sentence)\n",
    "        total_fre += textstat.flesch_reading_ease(sentence)\n",
    "\n",
    "    avg_fkgl = total_fkgl / num_sentences if num_sentences > 0 else 0\n",
    "    avg_gfi = total_gfi / num_sentences if num_sentences > 0 else 0\n",
    "    avg_fre = total_fre / num_sentences if num_sentences > 0 else 0\n",
    "\n",
    "    return avg_fkgl, avg_gfi, avg_fre\n",
    "\n",
    "# Example list of sentences\n",
    "sentences = qwen_1by1_list\n",
    "\n",
    "# Evaluate readability metrics for the list of sentences\n",
    "avg_fkgl, avg_gfi, avg_fre = evaluate_readability_metrics(sentences)\n",
    "print(\"Average FKGL:\", avg_fkgl)\n",
    "print(\"Average GFI:\", avg_gfi)\n",
    "print(\"Average FRE:\", avg_fre)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Program Files\\KPA prompt-based\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 22.849023818969727\n"
     ]
    }
   ],
   "source": [
    "#Perplexity 不合适\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Define a custom dataset class for the test dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, max_length):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(model, tokenizer, test_loader):\n",
    "    total_loss = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        input_ids = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            logits = outputs.logits\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = input_ids[..., 1:].contiguous()\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            num_samples += input_ids.size(0)\n",
    "\n",
    "    perplexity = torch.exp(torch.tensor(total_loss / num_samples)).item()\n",
    "    return perplexity\n",
    "\n",
    "# Example test sentences\n",
    "test_sentences = gt_list\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Create test dataset and DataLoader\n",
    "test_dataset = TestDataset(test_sentences, tokenizer, max_length=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = calculate_perplexity(model, tokenizer, test_loader)\n",
    "print(\"Perplexity:\", perplexity)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Program Files\\KPA prompt-based\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "2.0299108655308373e-05\n",
      "1: \n",
      "2.1840392946614884e-05\n",
      "2: \n",
      "1.981463901756797e-05\n",
      "3: \n",
      "1.969588265637867e-05\n",
      "4: \n",
      "1.2305537893553264e-05\n",
      "5: \n",
      "3.392390794942912e-07\n",
      "6: \n",
      "3.211306830053218e-05\n",
      "7: \n",
      "2.346013161513838e-06\n",
      "8: \n",
      "4.874080445915752e-07\n",
      "9: \n",
      "1.1166133617734886e-06\n",
      "10: \n",
      "0.00015527517825830728\n",
      "11: \n",
      "6.705774467263836e-06\n",
      "12: \n",
      "0.0016358387656509876\n",
      "13: \n",
      "0.00010472341818967834\n",
      "14: \n",
      "2.0935411157552153e-05\n",
      "15: \n",
      "1.2497412171796896e-05\n",
      "16: \n",
      "2.452197077218443e-05\n",
      "17: \n",
      "1.3434158063319046e-05\n",
      "18: \n",
      "1.5494300896534696e-05\n",
      "19: \n",
      "1.716083897917997e-05\n",
      "20: \n",
      "1.6787045751698315e-05\n",
      "21: \n",
      "2.3265720301424153e-05\n",
      "22: \n",
      "1.1671898391796276e-05\n",
      "23: \n",
      "1.7566750102560036e-05\n",
      "24: \n",
      "1.3698321708943695e-05\n",
      "25: \n",
      "0.0003159624757245183\n",
      "26: \n",
      "3.6815329167438904e-06\n",
      "27: \n",
      "2.690727114895708e-06\n",
      "28: \n",
      "4.322956101532327e-06\n",
      "29: \n",
      "3.1695893994765356e-05\n",
      "30: \n",
      "1.0426081644254737e-05\n",
      "31: \n",
      "2.0875093468930572e-05\n",
      "32: \n",
      "1.6948642951319925e-05\n",
      "33: \n",
      "1.1690677638398483e-05\n",
      "34: \n",
      "1.115234408644028e-05\n",
      "35: \n",
      "0.00021599885076284409\n",
      "36: \n",
      "2.331301038793754e-05\n",
      "37: \n",
      "0.00010488171392353252\n",
      "38: \n",
      "9.824096923694015e-05\n",
      "39: \n",
      "2.0918545487802476e-05\n",
      "40: \n",
      "1.4540160009346437e-07\n",
      "41: \n",
      "4.812269253307022e-06\n",
      "42: \n",
      "1.2685539331869222e-05\n",
      "43: \n",
      "0.00017604167805984616\n",
      "44: \n",
      "1.5571298717986792e-05\n",
      "45: \n",
      "3.0665680242236704e-05\n",
      "46: \n",
      "7.007102976785973e-05\n",
      "47: \n",
      "2.5747945983312093e-05\n",
      "48: \n",
      "7.068815466482192e-05\n",
      "49: \n",
      "1.850502121669706e-05\n",
      "50: \n",
      "7.469885076716309e-06\n",
      "51: \n",
      "3.985131115769036e-06\n",
      "52: \n",
      "1.0952233424177393e-05\n",
      "53: \n",
      "1.3270309864310548e-05\n",
      "54: \n",
      "7.989982805156615e-06\n",
      "55: \n",
      "1.2557636182464194e-05\n",
      "56: \n",
      "0.00027528713690117\n",
      "57: \n",
      "0.0002870172611437738\n",
      "58: \n",
      "4.199559455742019e-08\n",
      "59: \n",
      "1.7675662093097344e-05\n",
      "60: \n",
      "1.4623069546360057e-05\n",
      "61: \n",
      "1.359734778816346e-05\n",
      "62: \n",
      "1.807717671908904e-05\n",
      "63: \n",
      "1.545440318295732e-05\n",
      "64: \n",
      "2.6382644136901945e-05\n",
      "65: \n",
      "1.2331228390394244e-05\n",
      "66: \n",
      "1.491913280915469e-05\n",
      "67: \n",
      "2.0391469661262818e-05\n",
      "68: \n",
      "1.924380376294721e-05\n",
      "69: \n",
      "1.4343962902785279e-05\n",
      "70: \n",
      "2.25803378270939e-05\n",
      "71: \n",
      "2.4367203877773136e-05\n",
      "72: \n",
      "1.9005947251571342e-05\n",
      "73: \n",
      "1.5189936675596982e-05\n",
      "74: \n",
      "1.1593656381592155e-05\n",
      "75: \n",
      "1.5201977475953754e-05\n",
      "76: \n",
      "2.159164250770118e-05\n",
      "77: \n",
      "1.587605038366746e-05\n",
      "78: \n",
      "2.2855323550174944e-05\n",
      "79: \n",
      "1.9863533452735282e-05\n",
      "80: \n",
      "8.204874575312715e-06\n",
      "81: \n",
      "0.0009095396380871534\n",
      "82: \n",
      "0.0001132476536440663\n",
      "83: \n",
      "1.3307075050761341e-06\n",
      "84: \n",
      "1.2733166840916965e-05\n",
      "85: \n",
      "9.867922017292585e-06\n",
      "86: \n",
      "1.3917015166953206e-05\n",
      "87: \n",
      "1.9441409676801413e-05\n",
      "88: \n",
      "1.475886983826058e-05\n",
      "89: \n",
      "8.397138117288705e-06\n",
      "90: \n",
      "6.287924406933598e-06\n",
      "91: \n",
      "2.4350776584469713e-05\n",
      "92: \n",
      "4.333607648732141e-06\n",
      "93: \n",
      "9.021677556120267e-07\n",
      "94: \n",
      "1.7708709492580965e-05\n",
      "95: \n",
      "2.045559449470602e-05\n",
      "96: \n",
      "1.7922640836331993e-05\n",
      "97: \n",
      "2.0939105525030755e-05\n",
      "98: \n",
      "4.3493902921909466e-05\n",
      "99: \n",
      "1.7945019862963818e-05\n",
      "100: \n",
      "9.618353942641988e-06\n",
      "101: \n",
      "1.9884557332261465e-05\n",
      "102: \n",
      "1.3191614016250242e-05\n",
      "103: \n",
      "2.0757670426974073e-05\n",
      "104: \n",
      "1.2985316061531194e-05\n",
      "105: \n",
      "2.1460258722072467e-05\n",
      "106: \n",
      "1.5553083358099684e-05\n",
      "107: \n",
      "2.0645902623073198e-05\n",
      "108: \n",
      "1.2156387128925417e-05\n",
      "109: \n",
      "2.6610005079419352e-05\n",
      "110: \n",
      "4.5866352593293414e-05\n",
      "111: \n",
      "4.4801749936596025e-06\n",
      "112: \n",
      "7.98970560822454e-08\n",
      "113: \n",
      "5.46204719285015e-05\n",
      "114: \n",
      "1.1178402928635478e-05\n",
      "115: \n",
      "2.3402524675475433e-05\n",
      "116: \n",
      "0.005352762062102556\n",
      "117: \n",
      "0.00018713534518610686\n",
      "118: \n",
      "0.00014700043539050967\n",
      "119: \n",
      "2.997895353473723e-05\n",
      "120: \n",
      "2.2245470972848125e-05\n",
      "121: \n",
      "2.7234053050051443e-05\n",
      "122: \n",
      "3.402485526748933e-05\n",
      "123: \n",
      "1.3075539754936472e-05\n",
      "124: \n",
      "2.3730130124022253e-05\n",
      "125: \n",
      "5.710082405130379e-06\n",
      "126: \n",
      "0.000948850647546351\n",
      "127: \n",
      "0.00016165092529263347\n",
      "128: \n",
      "0.0004831633414141834\n",
      "129: \n",
      "2.8835459033871302e-06\n",
      "130: \n",
      "1.8765060303849168e-05\n",
      "131: \n",
      "1.3376210517890286e-05\n",
      "132: \n",
      "2.2345069737639278e-05\n",
      "133: \n",
      "2.570706965343561e-05\n",
      "134: \n",
      "2.1073206880828366e-05\n",
      "135: \n",
      "1.3717723049921915e-05\n",
      "136: \n",
      "2.690866131160874e-05\n",
      "137: \n",
      "1.1072545021306723e-05\n",
      "138: \n",
      "2.0133162252022885e-05\n",
      "139: \n",
      "2.4662500436534174e-05\n",
      "140: \n",
      "1.4566257959813811e-05\n",
      "141: \n",
      "1.1919712051167153e-05\n",
      "142: \n",
      "1.0024617949966341e-05\n",
      "143: \n",
      "1.1949036888836417e-05\n",
      "144: \n",
      "1.1800096217484679e-05\n",
      "145: \n",
      "0.000674729177262634\n",
      "146: \n",
      "9.55873019847786e-06\n",
      "147: \n",
      "1.0729831956268754e-05\n",
      "148: \n",
      "0.00017347368702758104\n",
      "149: \n",
      "8.94483946467517e-06\n",
      "150: \n",
      "1.6702882930985652e-05\n",
      "151: \n",
      "1.5250672902311635e-07\n",
      "152: \n",
      "5.918826673223521e-07\n",
      "153: \n",
      "0.00297303544357419\n",
      "154: \n",
      "1.666215030127205e-05\n",
      "155: \n",
      "2.542522224757704e-06\n",
      "156: \n",
      "0.00025351697695441544\n",
      "157: \n",
      "1.123792912949284e-06\n",
      "158: \n",
      "2.467447666276712e-05\n",
      "159: \n",
      "3.0639359465567395e-05\n",
      "160: \n",
      "1.3894107723899651e-05\n",
      "161: \n",
      "1.625092227186542e-05\n",
      "162: \n",
      "2.4307311832671985e-05\n",
      "163: \n",
      "1.364813851978397e-05\n",
      "164: \n",
      "1.0813871995196678e-05\n",
      "165: \n",
      "1.1495123544591479e-05\n",
      "166: \n",
      "2.1479585484485142e-05\n",
      "167: \n",
      "1.2558935850393027e-05\n",
      "168: \n",
      "1.4383665075001772e-05\n",
      "169: \n",
      "9.431219950783998e-06\n",
      "170: \n",
      "2.299212610523682e-06\n",
      "171: \n",
      "4.173794877715409e-06\n",
      "172: \n",
      "2.7156131181982346e-05\n",
      "173: \n",
      "5.6303342717001215e-05\n",
      "174: \n",
      "2.3288328520720825e-05\n",
      "175: \n",
      "1.6065861927927472e-05\n",
      "176: \n",
      "1.7958418538910337e-05\n",
      "177: \n",
      "1.6562084056204185e-05\n",
      "178: \n",
      "1.639210677240044e-05\n",
      "179: \n",
      "1.8462154912413098e-05\n",
      "180: \n",
      "0.0002588901261333376\n",
      "181: \n",
      "4.504650132730603e-05\n",
      "182: \n",
      "7.321912562474608e-05\n",
      "183: \n",
      "6.26936525804922e-06\n",
      "184: \n",
      "2.1508524241653504e-06\n",
      "185: \n",
      "0.0002425120328553021\n",
      "186: \n",
      "0.00010172741167480126\n",
      "187: \n",
      "8.009098564798478e-06\n",
      "188: \n",
      "6.636214493482839e-06\n",
      "189: \n",
      "1.651781531109009e-05\n",
      "190: \n",
      "1.4678600564366207e-05\n",
      "191: \n",
      "1.3879180187359452e-05\n",
      "192: \n",
      "9.7019255917985e-06\n",
      "193: \n",
      "1.853173307608813e-05\n",
      "194: \n",
      "1.3143091564415954e-05\n",
      "195: \n",
      "5.3086750995134935e-05\n",
      "196: \n",
      "5.2656100706371944e-06\n",
      "197: \n",
      "0.0001551596651552245\n",
      "198: \n",
      "1.7147300468423055e-06\n",
      "199: \n",
      "1.5653364243917167e-05\n",
      "200: \n",
      "1.4685519090562593e-05\n",
      "201: \n",
      "1.0268136065860745e-05\n",
      "202: \n",
      "1.6489522749907337e-05\n",
      "203: \n",
      "1.8782380720949732e-05\n",
      "204: \n",
      "1.1957878086832352e-05\n",
      "205: \n",
      "1.4047543118067551e-05\n",
      "206: \n",
      "2.1994834241922945e-05\n",
      "207: \n",
      "1.1112199899798725e-05\n",
      "208: \n",
      "1.5225605238811113e-05\n",
      "209: \n",
      "1.0778227988339495e-05\n",
      "210: \n",
      "3.296252907603048e-05\n",
      "211: \n",
      "0.00012786014121957123\n",
      "212: \n",
      "1.371387611470709e-06\n",
      "213: \n",
      "0.00019047201203648\n",
      "214: \n",
      "1.4440542145166546e-05\n",
      "215: \n",
      "1.3918389413447585e-05\n",
      "216: \n",
      "2.2529338821186684e-05\n",
      "217: \n",
      "1.6999807485262863e-05\n",
      "218: \n",
      "3.219414793420583e-05\n",
      "219: \n",
      "1.4720022591063753e-05\n",
      "220: \n",
      "8.820747439131083e-07\n",
      "221: \n",
      "7.3324499680893496e-06\n",
      "222: \n",
      "1.04375021692249e-05\n",
      "223: \n",
      "6.72488022246398e-06\n",
      "224: \n",
      "2.19507965084631e-05\n",
      "225: \n",
      "2.5982579245464876e-05\n",
      "226: \n",
      "2.1983414626447484e-05\n",
      "227: \n",
      "1.101326233765576e-06\n",
      "228: \n",
      "0.00019065054948441684\n",
      "229: \n",
      "6.389807367668254e-06\n",
      "230: \n",
      "3.1774328817846254e-05\n",
      "231: \n",
      "1.4490491594187915e-05\n",
      "232: \n",
      "0.0022271345369517803\n",
      "233: \n",
      "1.8089756849803962e-05\n",
      "234: \n",
      "2.6796056772582233e-05\n",
      "235: \n",
      "4.980641961083165e-07\n",
      "236: \n",
      "4.6018049033591524e-05\n",
      "237: \n",
      "2.4915994799812324e-05\n",
      "238: \n",
      "0.00017609326459933072\n",
      "239: \n",
      "1.2450918802642263e-05\n",
      "240: \n",
      "0.0005911989719606936\n",
      "241: \n",
      "7.33354827389121e-05\n",
      "242: \n",
      "9.921759919961914e-05\n",
      "243: \n",
      "4.11163136959658e-06\n",
      "244: \n",
      "5.549445631913841e-05\n",
      "245: \n",
      "1.6782989405328408e-05\n",
      "246: \n",
      "2.177101850975305e-05\n",
      "247: \n",
      "2.524319825170096e-05\n",
      "248: \n",
      "2.9805847589159384e-05\n",
      "249: \n",
      "1.9380882804398425e-05\n",
      "250: \n",
      "1.888480255729519e-05\n",
      "251: \n",
      "1.1705749784596264e-05\n",
      "252: \n",
      "1.7753334759618156e-05\n",
      "253: \n",
      "7.944305480123148e-07\n",
      "254: \n",
      "1.1975612324022222e-05\n",
      "255: \n",
      "1.5365874787676148e-05\n",
      "256: \n",
      "1.7727288650348783e-06\n",
      "257: \n",
      "1.4945466318749823e-05\n",
      "258: \n",
      "5.078214599052444e-06\n",
      "259: \n",
      "2.2692700440529734e-05\n",
      "260: \n",
      "1.5928188076941296e-05\n",
      "261: \n",
      "1.5911735317786224e-05\n",
      "262: \n",
      "2.1469046259880997e-05\n",
      "263: \n",
      "1.1103926226496696e-05\n",
      "264: \n",
      "3.087536970269866e-05\n",
      "265: \n",
      "1.4324616131489165e-05\n",
      "266: \n",
      "8.747509127715603e-06\n",
      "267: \n",
      "1.1661950338748284e-05\n",
      "268: \n",
      "9.568469067744445e-06\n",
      "269: \n",
      "1.10984901766642e-05\n",
      "270: \n",
      "0.001897904323413968\n",
      "271: \n",
      "0.0001438756298739463\n",
      "272: \n",
      "0.0001896301400847733\n",
      "273: \n",
      "4.351195821072906e-05\n",
      "274: \n",
      "4.62133311884827e-06\n",
      "275: \n",
      "1.9125753169646487e-05\n",
      "276: \n",
      "1.1752806130971294e-05\n",
      "277: \n",
      "1.1734361578419339e-05\n",
      "278: \n",
      "1.834219801821746e-05\n",
      "279: \n",
      "1.194315427710535e-05\n",
      "280: \n",
      "5.2810264605795965e-06\n",
      "281: \n",
      "3.213406671420671e-05\n",
      "282: \n",
      "8.583564522268716e-06\n",
      "283: \n",
      "8.976941899163648e-06\n",
      "284: \n",
      "2.0652856619562954e-05\n",
      "285: \n",
      "7.1347958510159515e-06\n",
      "286: \n",
      "1.1078725492552621e-08\n",
      "287: \n",
      "2.7423209758126177e-05\n",
      "288: \n",
      "8.204361802199855e-05\n",
      "289: \n",
      "1.1813343917310704e-05\n",
      "290: \n",
      "7.2316561272600666e-06\n",
      "291: \n",
      "1.3314008356246632e-05\n",
      "292: \n",
      "1.674458326306194e-05\n",
      "293: \n",
      "1.360443184239557e-05\n",
      "294: \n",
      "2.0047604266437702e-05\n",
      "295: \n",
      "1.4611566257372033e-05\n",
      "296: \n",
      "1.8251572328153998e-05\n",
      "297: \n",
      "9.308218977821525e-06\n",
      "298: \n",
      "1.0808564184117131e-05\n",
      "299: \n",
      "1.4832728084002156e-05\n",
      "300: \n",
      "0.00022192629694472998\n",
      "301: \n",
      "3.81017271138262e-05\n",
      "302: \n",
      "1.8071643353323452e-05\n",
      "303: \n",
      "4.84584343212191e-05\n",
      "304: \n",
      "3.5367676900932565e-05\n",
      "305: \n",
      "0.0011101356940343976\n",
      "306: \n",
      "0.00012309171142987907\n",
      "307: \n",
      "6.9214006543916184e-06\n",
      "308: \n",
      "0.00026904954575002193\n",
      "309: \n",
      "1.577441071276553e-05\n",
      "Average fluency score: 9.260513978063733e-05\n"
     ]
    }
   ],
   "source": [
    "#GPT2 fluency\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "def calculate_fluency(sentence):\n",
    "    # Load pre-trained GPT-2 model and tokenizer\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the sentence\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "\n",
    "    # Generate logits for the next token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "    # Compute softmax to get probabilities\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # Get the probability of the actual token (last token in input_ids)\n",
    "    last_token_prob = probabilities[0, input_ids[0, -1]].item()\n",
    "\n",
    "    return last_token_prob\n",
    "\n",
    "def evaluate_fluency(sentences):\n",
    "    total_fluency = 0\n",
    "    num_sentences = len(sentences)\n",
    "    num = 0\n",
    "    for sentence in sentences:\n",
    "        fluency_score = calculate_fluency(sentence)\n",
    "        print(str(num)+\": \")\n",
    "        print(fluency_score)\n",
    "        num += 1\n",
    "        total_fluency += fluency_score\n",
    "\n",
    "    average_fluency = total_fluency / num_sentences if num_sentences > 0 else 0\n",
    "\n",
    "    return average_fluency\n",
    "\n",
    "# Example list of sentences\n",
    "sentences = gpt4_group_list\n",
    "\n",
    "# Evaluate fluency for the list of sentences\n",
    "avg_fluency = evaluate_fluency(sentences)\n",
    "print(\"Average fluency score:\", avg_fluency)\n",
    "# 21.758671085682257e-05\n",
    "# 9.260513978063733e-05\n",
    "# 1.73425157821166e-05\n",
    "# 1.715389794445664e-05\n",
    "# 1.6184581795538716e-05\n",
    "# 7.567835783613581e-05\n",
    "# 1.915149998988073e-05"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 117.07741935483871\n"
     ]
    }
   ],
   "source": [
    "#length\n",
    "# Calculate the total length of all sentences\n",
    "a = qwen_1by1_list\n",
    "total_length = sum(len(sentence) for sentence in a)\n",
    "\n",
    "# Calculate the average length\n",
    "average_length = total_length / len(a)\n",
    "\n",
    "print(\"Average sentence length:\", average_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average grammaticality score: 0.04516129032258064\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.3548387096774194"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grammar Score\n",
    "import language_tool_python\n",
    "\n",
    "def calculate_grammar_score(sentences):\n",
    "    # Initialize LanguageTool\n",
    "    tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "    total_score = 0\n",
    "    total_sentences = 0\n",
    "\n",
    "    # Iterate over each sentence\n",
    "    for sentence in sentences:\n",
    "        # Check grammar for the sentence\n",
    "        matches = tool.check(sentence)\n",
    "        # Calculate the grammar score for the sentence\n",
    "        score = len(matches)\n",
    "        # Increment the total score\n",
    "        total_score += score\n",
    "        # Increment the total number of sentences\n",
    "        total_sentences += 1\n",
    "\n",
    "    # Calculate the average grammaticality score\n",
    "    if total_sentences > 0:\n",
    "        avg_grammaticality_score = total_score / total_sentences\n",
    "    else:\n",
    "        avg_grammaticality_score = 0\n",
    "\n",
    "    return avg_grammaticality_score\n",
    "\n",
    "# Example list of sentences\n",
    "sentences = qwen_1by1_list\n",
    "# ,gpt4_1by1_list,llama2_group_list,llama2_1by1_list,qwen_group_list,qwen_1by1_list\n",
    "# Calculate the average grammaticality score\n",
    "avg_grammaticality = calculate_grammar_score(sentences)\n",
    "print(\"Average grammaticality score:\", avg_grammaticality)\n",
    "0.11956521739130435\n",
    "0.05806451612903226\n",
    "0.04516129032258064\n",
    "0.06774193548387097\n",
    "0.04516129032258064\n",
    "0.3548387096774194\n",
    "0.04516129032258064"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "Assisted suicide equates to facilitating murder and should therefore be criminalized.\n",
      "Assisted suicide should be a criminal offense because it can lead to exploitation and murder of vulnerable individuals who may not have the capacity to make informed decisions.\n",
      "Assisted suicide constitutes glorified murder, hence it should be a criminal offence, with focus on providing support for better coping mechanisms.\n",
      "1: \n",
      "Assisting in suicide should be punishable by law as it further encourages desperate individuals to take their lives instead of seeking help.\n",
      "Assisted suicide is a form of murder and those who assist in it should be charged with the crime.\n",
      "Assisting in suicide constitutes murder and necessitates criminal punishment.\n",
      "2: \n",
      "Assisted suicide is unethical because taking a life, for any reason, is wrong.\n",
      "Assisted suicide should be a criminal offense because it goes against the fundamental value of preserving human life and can lead to exploitation and abuse.\n",
      "The belief that assisted suicide should be criminalized stems from the premise that it is currently illegal.\n",
      "3: \n",
      "The user supports the criminalization of assisted suicide due to its illegality.\n",
      "Assisted suicide should be a criminal offense as it goes against God's law and the belief that He has ultimate authority over life and death.\n",
      "Criminalizing assisted suicide is essential to safeguard vulnerable individuals from being exploited for financial gain by greedy relatives.\n",
      "4: \n",
      "Assisted suicide should be a criminal offense due to the intention to end a human life.\n",
      "Assisted suicide raises concerns about the potential for abuse and exploitation, particularly towards vulnerable individuals who may feel pressured to end their lives.\n",
      "Assisted suicide is equated with murder.\n",
      "5: \n",
      "Assisted suicide provides a more peaceful death for terminally-ill individuals who might otherwise suffer or cause more harm by attempting suicide on their own.\n",
      "The user's query suggests that assisted suicide should be a criminal offense because it goes against the idea of helping people end their lives in a \"civil way\" when they are willing to die.\n",
      "Assisted suicide should not be criminalized due to its role in alleviating unbearable suffering for terminally ill patients.\n",
      "6: \n",
      "Assisted suicide provides the option for terminally ill individuals to end their pain and suffering.\n",
      "Assisted suicide should be a criminal offense because it undermines the value of human life and can lead to exploitation and abuse.\n",
      "Assisted suicide should not be criminalized as it provides relief to terminally ill patients suffering unbearable pain.\n",
      "7: \n",
      "Assisted suicide should not be criminalized if the patient's critical health condition is irreversible.\n",
      "Individuals with terminal illnesses or unbearable medical conditions should have the autonomy to choose assisted suicide, and medical professionals who aid them should not face criminalization.\n",
      "Assisted suicide should remain an option for those with no alternative recourse.\n",
      "8: \n",
      "The decision to end one's life should be respected, particularly in cases of terminal illness.\n",
      "Assisted suicide should be a criminal offense because it can lead to abuse and exploitation of vulnerable individuals who may feel pressured to end their lives against their will.\n",
      "Assisted suicide provides relief for terminally ill patients nearing end of life.\n",
      "9: \n",
      "Assisting someone who willingly wants to die should not be considered a criminal offence.\n",
      "Assisted suicide should be legalized as it is a human right to choose not to suffer with a terminal illness and live with dignity.\n",
      "Voluntary euthanasia should be legalized to respect individuals' autonomy in ending their suffering.\n"
     ]
    }
   ],
   "source": [
    "for a in range(10):\n",
    "    print(str(a)+\": \")\n",
    "    print(gpt4_1by1_list[a])\n",
    "    print(llama2_1by1_list[a])\n",
    "    print(qwen_1by1_list[a])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "0.04516129032258064"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}